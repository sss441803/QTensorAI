{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QTensorAI Tutorial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start using QTensorAI for your own hybrid quantum-classical neural network research, you need to do a few things.\n",
    "1. Write your own circuit composer.\n",
    "2. Write your own hybrid pytorch neural network module.\n",
    "\n",
    "The circuit composer is what you use to create your circuit, and the hybrid module is what you use to integrate with a machine learning pipeline.\n",
    "\n",
    "We have built base classes of circuit composers and hybrid modules that allows you to focus on the science of your study, rather than the implementation details of our library."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Circuit Composer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to create a straight forward quantum neural network as is described by Abbas et. al. in arXiv:2011.00027. We will not focus on the circuit, but on how you can use our library. First, let us import the class that facilitates your creation of a custom circuit composer. This class is the `ParallelComposer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minzhaoliu/.conda/envs/qtensor_ai/lib/python3.11/site-packages/tqdm-4.65.0-py3.11.egg/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from qtensor_ai import ParallelComposer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can create our own composer inheriting from the `ParallelComposer` class. This composer is also provided in `examples/Custom_Circuit_Composers.py`. The comments in this notebook are made with a narrative flow and is best to read without skipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNNComposer(ParallelComposer):\n",
    "    \n",
    "    '''For initialization, n_qubits is always needed since the ParallelComposer class requires that\n",
    "    The other parameters are specific to this example and are not important.'''\n",
    "    def __init__(self, n_qubits, n_layers, higher_order=False):\n",
    "        self.n_layers = n_layers\n",
    "        self.higher_order = higher_order\n",
    "        super().__init__(n_qubits)\n",
    "    \n",
    "    '''This creates a layer of Hadamard gates. To apply any gates,\n",
    "    use self.apply_gate(self.operators.gate, *qubits, **parameters).\n",
    "    There can be mulple qubits or parameters, depending on the gate type.\n",
    "    Other functions below are logics to add gates. We can ignore them for now.'''\n",
    "    def layer_of_Hadamards(self):\n",
    "        for q in self.qubits:\n",
    "            self.apply_gate(self.operators.H, q)\n",
    "    \n",
    "    def entangling_layer(self):\n",
    "        for i in range(self.n_qubits//2):\n",
    "            control_qubit = self.qubits[2*i]\n",
    "            target_qubit = self.qubits[2*i+1]\n",
    "            self.apply_gate(self.operators.cX, control_qubit, target_qubit)\n",
    "        for i in range((self.n_qubits+1)//2-1):\n",
    "            control_qubit = self.qubits[2*i+1]\n",
    "            target_qubit = self.qubits[2*i+2]\n",
    "            '''For example, you can put two qubits for CNOT (cX) gates.'''\n",
    "            self.apply_gate(self.operators.cX, control_qubit, target_qubit)\n",
    "        control_qubit = self.qubits[-1]\n",
    "        target_qubit = self.qubits[0]\n",
    "    \n",
    "    def encoding_circuit(self, data):\n",
    "        self.layer_of_Hadamards()\n",
    "        for i, qubit in enumerate(self.qubits):\n",
    "            self.apply_gate(self.operators.ZPhase, qubit, alpha=data[:, i])\n",
    "        if self.higher_order:\n",
    "            for i in range(self.n_qubits):\n",
    "                for j in range(i+1, self.n_qubits):\n",
    "                    control_qubit = self.qubits[i]\n",
    "                    target_qubit = self.qubits[j]\n",
    "                    self.apply_gate(self.operators.cX, control_qubit, target_qubit)\n",
    "                    self.apply_gate(self.operators.ZPhase, target_qubit, alpha=data[:, i]*data[:, j])\n",
    "                    self.apply_gate(self.operators.cX, control_qubit, target_qubit)\n",
    "    \n",
    "    def variational_layer(self, layer, layer_params):\n",
    "        for i in range(self.n_qubits):\n",
    "            qubit = self.qubits[i]\n",
    "            '''For an RY (YPhase) gate, there is one qubit,\n",
    "            and a alpha parameter which is a torch.Tensor of size (n_batch, 1).'''\n",
    "            self.apply_gate(self.operators.YPhase, qubit, alpha=layer_params[:, i])\n",
    "     \n",
    "    def cost_operator(self):\n",
    "        for qubit in self.qubits:\n",
    "            self.apply_gate(self.operators.Z, qubit)\n",
    "\n",
    "    def forward_circuit(self, data, params):\n",
    "        self.encoding_circuit(data)\n",
    "        self.entangling_layer()\n",
    "        for layer in range(self.n_layers):\n",
    "            self.variational_layer(layer, params[:, :, layer])\n",
    "            self.entangling_layer()\n",
    "    '''The detailed inputs of apply_gate will depend on the gate, and you can implement\n",
    "    custom gates as well. For those, you could ask for any fancy input parameters.\n",
    "\n",
    "    Moving on, with all the functions for building the circuit ready, we can implement\n",
    "    required functions. Specifically, any custom composers must have update_full_circuit\n",
    "    and name.\n",
    "\n",
    "    This function builds circuit whose first amplitude is the expectation value of\n",
    "    the measured circuit w.r.t. the cost_operator.\n",
    "    This function needs to return the circuit (a list) whose first amplitude you want to simulate.'''\n",
    "    def updated_full_circuit(self, **parameters):\n",
    "        data = parameters['data']\n",
    "        params = parameters['params']\n",
    "        '''All the apply_gate operations actually appends the gates to self.builder.circuit.\n",
    "        Hence, after we call the functions that builds the circuits, wee need to fetch the\n",
    "        circuit from self.builder. We will also need to clean the builder up from time to time.'''\n",
    "        self.builder.reset() # Clear builder.circuit\n",
    "        self.forward_circuit(data, params) # Set builder.circuit to the forward circuit according to data and params\n",
    "        self.cost_operator() # Add the cost operators to builder.circuit\n",
    "        first_part = self.builder.circuit # Extract builder.circuit at this stage for later use\n",
    "        self.builder.reset() # Clear builder.circuit\n",
    "        self.forward_circuit(data, params) # Set builder.circuit to the forward circuit according to data and params\n",
    "        self.builder.inverse() # Change builder.circuit to it's reverse, which is the forward circuit in reverse\n",
    "        second_part = self.builder.circuit # Extract the inverse circuit\n",
    "        self.builder.reset() # Clear builder circuit\n",
    "        '''The final circuit is forward + cost + inverse.\n",
    "        The first amplitude is the expectation value of the cost operator\n",
    "        for the forward circuit initialized with the 0 state.'''\n",
    "        return first_part + second_part\n",
    "    '''Although this function returns a circuit, we should not call this function in general.\n",
    "    This is because the parent class uses this function for the produce_circuit method to\n",
    "    generate the circuit and tensors on the GPU behind the scene.'''\n",
    "\n",
    "    '''This function returns the name of the circuit composer'''\n",
    "    def name(self):\n",
    "        return 'QNN'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, you need to initialize the `ParallelComposer` class with `n_qubits`, call `self.apply_gate` to add gates to `self.builder.circuit`, and create the `update_full_circuit` method which returns the circuit whose first amplitude you want to simulate. Finally, create the `name` method.\n",
    "\n",
    "## Custom Hybrid Module\n",
    "\n",
    "Now, let us move on to creating a custom hybrid module that can interface with a machine learning pipeline. First, import the `HybridModule` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qtensor_ai import HybridModule, DefaultOptimizer\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also include the `DefaultOptimizer` class to put in as the default choice for the optimizer. We will create a drop-in replacement of a classical fully connected layer called `QNN`. This can also be found in `examples/Custom_Modules.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This is a drop-in replacement of linear layers.\n",
    "The number of input features is the number of qubits.\n",
    "Each output feature is computed by an independently parameterized circuit'''\n",
    "class QNN(HybridModule):\n",
    "    \n",
    "    def __init__(self, in_features, out_features, variational_layers=1, higher_order=False, optimizer=DefaultOptimizer()):\n",
    "                \n",
    "        '''Initializing module parameters\n",
    "        The variable circuit_name is needed for initialization of the parent class HybridModule\n",
    "        All the other attributes are unique to this circuit and we can ignore.'''\n",
    "        circuit_name = 'n_{}_l_{}'.format(in_features, variational_layers)\n",
    "        self.higher_order = higher_order\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.variational_layers = variational_layers\n",
    "        self.higher_order = higher_order\n",
    "        \n",
    "        '''Define the circuit composer and initialize the hybrid module.\n",
    "        The composer is the custom composer we just defined.'''\n",
    "        composer = QNNComposer(in_features, variational_layers, higher_order=higher_order)\n",
    "        super(QNN, self).__init__(circuit_name=circuit_name, composer=composer, optimizer=optimizer)\n",
    "\n",
    "        '''self.weight are model weights. Weights must be defined after super().__init__()'''\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features, variational_layers, dtype=torch.float32))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''These lines of code are trying to manipulate the array to give the right input to the simulation.\n",
    "        The circuits with different parameters will be simulated in a batch parallel manner.\n",
    "        The 0-th dimension is the parallel batch dimension.'''\n",
    "        n_batch = x.shape[0] # (n_batch, in_features)\n",
    "        # Because for multiple output features, the parallelism is in the batch dimension as well as the\n",
    "        # output feature size dimension, we need to make the 0-th dimension of size out_features*n_batch\n",
    "        x = x.repeat(self.out_features, 1) # (out_features*n_batch, in_features)\n",
    "        params = self.weight.unsqueeze(1) # (out_features, 1, in_features, variational_layers)\n",
    "        params = params.expand(-1, n_batch, -1, -1) # (out_features, n_batch, in_features, variational_layers)\n",
    "        params = params.reshape(self.out_features*n_batch, self.in_features, self.variational_layers) # (out_features*n_batch, in_features, variational_layers)\n",
    "\n",
    "        '''The actual simulation must be run by calling the parent_forward method of the parent class. \n",
    "        The parameters should be the same parameters as those accepted by the circuit composer'''\n",
    "        out = self.parent_forward(data=x, params=params) # (out_features*n_batch)\n",
    "        # Reshaping the outputs\n",
    "        out = torch.real(out) # (out_features*n_batch)\n",
    "        out = out.reshape(self.out_features, n_batch) # (out_features, n_batch)\n",
    "        out = out.permute(1, 0) # (n_batch, out_features)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, we need to have a `circuit_name`, initialize the `HybridModule` parent class with your circuit name, composer and optimizer, and only after super init create any model weights. Finally, to run the simulation, `self.parent_forward` must be used in `forward`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build a quantum convolutional module with the QNN module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This is an example for 1D convolution. The filter is replaced with the QNN.'''\n",
    "class QConv2D(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, variational_layers=1, higher_order=False, optimizer=DefaultOptimizer(), dilation=(1, 1), padding=(0, 0), stride=(1, 1)):\n",
    "        super().__init__()\n",
    "                \n",
    "        '''Initializing module parameters'''\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_qubits = in_channels * kernel_size[0] * kernel_size[1]\n",
    "        \n",
    "        '''Defining unfold operation for convolution'''\n",
    "        self.dilation = dilation\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.unfold = nn.Unfold(kernel_size=kernel_size, dilation=dilation, padding=padding, stride=stride)\n",
    "        \n",
    "        '''Defining multichannel filter to be convolved'''\n",
    "        self.kernel = QNN(self.n_qubits, out_channels, variational_layers, higher_order, optimizer)\n",
    "        \n",
    "\n",
    "    '''This function transforms batched, multichannel sequences into parallel values of convolution kernel inputs'''\n",
    "    def memory_strided_im2col(self, x):\n",
    "        # x has dimension (n_batch, in_channels, length1, length2)\n",
    "        out = self.unfold(x)\n",
    "        out = torch.transpose(out, 1, 2)\n",
    "        # out has dimension (n_batch, L, kernel_size[0]*kernel_size[1]*in_channels=n_qubits)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        kernel_size = self.kernel_size\n",
    "        padding = self.padding\n",
    "        dilation = self.dilation\n",
    "        stride = self.stride\n",
    "        n_batch = x.size(0) # (n_batch, in_channels, length1, length2)\n",
    "        h_in = x.size(2)\n",
    "        w_in = x.size(3)\n",
    "        x = self.memory_strided_im2col(x) # (n_batch, L, kernel_size[0]*kernel_size[1]*in_channels=n_qubits)\n",
    "        x = x.reshape(-1, kernel_size[0]*kernel_size[1]*self.in_channels) # (n_batch*L, kernel_size*in_channels=n_qubits)\n",
    "        output = self.kernel(x) # (n_batch*L, out_channels)  \n",
    "        output = output.reshape(n_batch, -1, self.out_channels) # (n_batch, L, out_channels) \n",
    "        output = output.transpose(1, 2) # (n_batch, out_channels, L)\n",
    "        h_out = floor((h_in + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1)\n",
    "        w_out = floor((w_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1)\n",
    "        output = torch.nn.functional.fold(output, (h_out, w_out), (1, 1))\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing Everything Together\n",
    "\n",
    "Now let us test if our model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f747d787510>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = QConv2D(1, 3, kernel_size=(3, 3))\n",
    "        self.conv2 = QConv2D(3, 3, kernel_size=(3, 3))\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(12, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 12)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "def train(epoch):\n",
    "  network.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = network(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), loss.item()))\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "      torch.save(network.state_dict(), './results/model.pth')\n",
    "      torch.save(optimizer.state_dict(), './results/optimizer.pth')\n",
    "\n",
    "def test():\n",
    "  network.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      output = network(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using previously saved contraction order at  <_io.BufferedReader name='/home/minzhaoliu/Saved_Contraction_Orders/OrderingOptimizer/QNN/n_9_l_1.pickle'>\n",
      "Using previously saved contraction order at  <_io.BufferedReader name='/home/minzhaoliu/Saved_Contraction_Orders/OrderingOptimizer/QNN/n_27_l_1.pickle'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_99656/3784323493.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "/home/minzhaoliu/.conda/envs/qtensor_ai/lib/python3.11/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3052, Accuracy: 980/10000 (10%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301146\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.296292\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.316158\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.324809\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m test()\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m   train(epoch)\n\u001b[1;32m      9\u001b[0m   test()\n",
      "Cell \u001b[0;32mIn[83], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     14\u001b[0m output \u001b[39m=\u001b[39m network(data)\n\u001b[1;32m     15\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnll_loss(output, target)\n\u001b[0;32m---> 16\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     17\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     18\u001b[0m \u001b[39mif\u001b[39;00m batch_idx \u001b[39m%\u001b[39m log_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/qtensor_ai/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/qtensor_ai/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir('./results'):\n",
    "    os.mkdir('./results')\n",
    "\n",
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch)\n",
    "  test()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9573c475ae947fc4c73ea79e643e7b87a0dff7cbdfd0f21335f7c021a88741d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
